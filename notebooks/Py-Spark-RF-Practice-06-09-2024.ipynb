{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638ffd64",
   "metadata": {},
   "source": [
    "# Spark with Python\n",
    "\n",
    "05-08-2023\n",
    "by Thomas Johnson III\n",
    "\n",
    "This notebook is to practice the usage of Pyspark for personal coding development. A lot of what is seen here will be extensions of existing work or methods for the loaded Python libraries that already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe50298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in necessary modules and functions.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression, GBTRegressor, RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import PCA\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb21a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some regression data to be used for fitting and evaluation.\n",
    "def gen_data():\n",
    "    np.random.seed(12) # Set a random seed\n",
    "    # Obtain the simulated data for regression.\n",
    "    dat_x, dat_y = make_regression(n_samples = 5000, # 5000 cases\n",
    "                                                   n_features = 25, # 25 variables, by default 10 are infromative.\n",
    "                                                   noise = 1)\n",
    "    x_names = []\n",
    "    for i in range(25):\n",
    "        x_names.append(\"X\" + str(i+1)) #Get names of X columns\n",
    "    # Dataframe conversion occurring\n",
    "    dat_x = pd.DataFrame(dat_x, columns = x_names) \n",
    "    dat_y = pd.DataFrame(dat_y, columns = [\"y\"])\n",
    "    return(dat_x, dat_y) # Return X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47851aeb",
   "metadata": {},
   "source": [
    "Now we obtain the $\\boldsymbol{X}$ and $\\boldsymbol{y}$ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7819315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dat, y_dat = gen_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1922797",
   "metadata": {},
   "source": [
    "Combine $\\boldsymbol{X}$ and \\boldsymbol{y} into one dataframe. Starting in pandas then moving the dataframe into pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a8e7f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_data = pd.concat([X_dat,y_dat], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc273b",
   "metadata": {},
   "source": [
    "Now that we have the population data, we can start loading into Pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1f0f82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 09:17:01 WARN Utils: Your hostname, LAPTOP-9T87KVBO resolves to a loopback address: 127.0.1.1; using 192.168.56.1 instead (on interface eth1)\n",
      "24/06/10 09:17:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/10 09:17:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Set up the Spark settings with 4Gb of memory. Set up a spark session to work in.\n",
    "# SparkContext.setSystemProperty(\"spark.executor.memory\", \"4g\").setSystemProperty(\"spark.cores.max\", \"6\")\n",
    "sp_context = SparkSession.builder.master(\"local[1]\").config(\"spark.executor.memory\", \"8g\").config(\"spark.cores.max\", \"6\") \\\n",
    "    .appName(\"ml_spark\").getOrCreate()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed963621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move pandas dataframe to Spark in memory\n",
    "pop_data_sp = sp_context.createDataFrame(pop_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343f32b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 09:17:06 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/06/10 09:17:07 WARN TaskSetManager: Stage 0 contains a task of very large size (1164 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "|summary|                  X2|                  X3|                  X4|                  X5|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  count|                5000|                5000|                5000|                5000|\n",
      "|   mean|-0.01210631494364...|-8.85369555971085...|-0.00268963742477...|-0.02023292955390...|\n",
      "| stddev|  0.9957392741360688|   1.006254205754425|  1.0083556524718054|  1.0015132191307523|\n",
      "|    min| -3.4157248243589216|  -3.654432921988915| -3.5675804306862626| -3.8897380263174064|\n",
      "|    25%|  -0.676277675641535| -0.6825850313187773| -0.6808199601427417| -0.6880649901638419|\n",
      "|    50%|-0.01392845608911...|4.529808944312057E-4|-0.01630454755484...|-0.01341902585471932|\n",
      "|    75%|  0.6391356777594667|  0.6768263942589848|  0.6704488113096037|  0.6501401096039792|\n",
      "|    max|   3.407902648447736|  3.8102699875673443|   3.570832933157062|    3.54798776514766|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate some brief summary statistics.\n",
    "pop_data_sp.select(pop_data_sp.columns[1:5]).summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b41b591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 09:17:10 WARN TaskSetManager: Stage 3 contains a task of very large size (1164 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-------------------+\n",
      "|summary|                 X24|                 X25|                  y|\n",
      "+-------+--------------------+--------------------+-------------------+\n",
      "|  count|                5000|                5000|               5000|\n",
      "|   mean|-0.00498021898128...|-0.02020631205235034| -3.798446493633077|\n",
      "| stddev|  0.9877367012402092|  1.0201279396655665| 234.20786791921896|\n",
      "|    min|  -3.367035150216294| -3.4850827883691182| -884.2724915998847|\n",
      "|    25%| -0.6889496650184208| -0.7074029537948439| -156.7858165392303|\n",
      "|    50%|-0.01406311451878...|-0.01767028251457...|-1.7103445920178768|\n",
      "|    75%|  0.6693079349083157|  0.6525044474585894| 158.57970199205099|\n",
      "|    max|   3.499212619752443|  3.8085663482139127|  972.4118959873122|\n",
      "+-------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calclate some brief summary statistics\n",
    "pop_data_sp.select(pop_data_sp.columns[23:]).summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9be3968-2d72-4a31-b0a7-5d2b8f70ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_data_sp = pop_data_sp.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bcfbde5-3470-4efe-9083-751aa3e9f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train test split.\n",
    "train_dat, test_dat = pop_data_sp.randomSplit(weights = [0.7, 0.3], seed = 424)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9474de8-24b3-48be-b815-827277ad3bee",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3628e3d-ca37-4bf5-9720-7e563cb78cf8",
   "metadata": {},
   "source": [
    "Adding a random forest as well for comparison with the GBT and lienar regression predicive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd18fa92-00b1-4b5e-be4e-98169d3a9357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest regressor model\n",
    "rfRegress = RandomForestRegressor(labelCol = \"y\", seed = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f32e34d9-b321-4902-bfce-d199563fd96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vector Assmbler to turn features columns into a vector for the machine learning methods\n",
    "vec_assembler = VectorAssembler(inputCols = pop_data_sp.columns[0:24],\n",
    "               outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb4637ed-3a35-46da-8d21-86a5e3789872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a pipeline\n",
    "rf_pipeline_1 = Pipeline(stages = [vec_assembler, rfRegress])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea9a0f0e-745d-4251-82e3-6f0160df2500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing the parameter grid\n",
    "rf_reg_params = ParamGridBuilder()\\\n",
    "    .addGrid(rfRegress.maxDepth, [5, 10])\\\n",
    "    .addGrid(rfRegress.maxBins, [16, 32])\\\n",
    "    .addGrid(rfRegress.numTrees, [10, 20, 30]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f8f4a10-f366-416a-bbf1-44695cddc8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building cross validaiton object\n",
    "rf_cross_val = CrossValidator(estimator = rf_pipeline_1,\n",
    "                              estimatorParamMaps = rf_reg_params,\n",
    "                              evaluator= RegressionEvaluator(labelCol = \"y\"),\n",
    "                                    numFolds = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36177f3b-e3f7-41a6-9785-67a953df9787",
   "metadata": {},
   "source": [
    "Get the time it takes to build the random forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81e333cd-eaad-4b42-8cc4-ea88b3b1031a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 09:19:18 WARN TaskSetManager: Stage 6 contains a task of very large size (1164 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/10 09:19:23 WARN TaskSetManager: Stage 31 contains a task of very large size (1164 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/10 09:19:41 WARN DAGScheduler: Broadcasting large task binary with size 1299.8 KiB\n",
      "24/06/10 09:19:47 WARN DAGScheduler: Broadcasting large task binary with size 1547.4 KiB\n",
      "24/06/10 09:19:48 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/06/10 09:19:55 WARN DAGScheduler: Broadcasting large task binary with size 1296.4 KiB\n",
      "24/06/10 09:19:56 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/06/10 09:19:58 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/06/10 09:20:06 WARN DAGScheduler: Broadcasting large task binary with size 1283.9 KiB\n",
      "24/06/10 09:20:12 WARN DAGScheduler: Broadcasting large task binary with size 1514.0 KiB\n",
      "24/06/10 09:20:14 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/06/10 09:20:20 WARN DAGScheduler: Broadcasting large task binary with size 1269.3 KiB\n",
      "24/06/10 09:20:22 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/06/10 09:20:24 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "24/06/10 09:20:29 WARN TaskSetManager: Stage 402 contains a task of very large size (1164 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/10 09:20:31 WARN TaskSetManager: Stage 427 contains a task of very large size (1164 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/10 09:20:48 WARN DAGScheduler: Broadcasting large task binary with size 1296.2 KiB\n",
      "24/06/10 09:20:54 WARN DAGScheduler: Broadcasting large task binary with size 1532.0 KiB\n",
      "24/06/10 09:20:55 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/06/10 09:21:01 WARN DAGScheduler: Broadcasting large task binary with size 1291.2 KiB\n",
      "24/06/10 09:21:02 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/06/10 09:21:04 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/06/10 09:21:11 WARN DAGScheduler: Broadcasting large task binary with size 1278.2 KiB\n",
      "24/06/10 09:21:17 WARN DAGScheduler: Broadcasting large task binary with size 1514.3 KiB\n",
      "24/06/10 09:21:19 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/06/10 09:21:26 WARN DAGScheduler: Broadcasting large task binary with size 1285.3 KiB\n",
      "24/06/10 09:21:28 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/06/10 09:21:31 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/06/10 09:21:34 WARN TaskSetManager: Stage 798 contains a task of very large size (1164 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/10 09:21:36 WARN TaskSetManager: Stage 823 contains a task of very large size (1164 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/10 09:21:52 WARN DAGScheduler: Broadcasting large task binary with size 1338.9 KiB\n",
      "24/06/10 09:21:58 WARN DAGScheduler: Broadcasting large task binary with size 1576.3 KiB\n",
      "24/06/10 09:21:59 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/06/10 09:22:05 WARN DAGScheduler: Broadcasting large task binary with size 1308.3 KiB\n",
      "24/06/10 09:22:06 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/06/10 09:22:08 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "24/06/10 09:22:16 WARN DAGScheduler: Broadcasting large task binary with size 1300.1 KiB\n",
      "24/06/10 09:22:22 WARN DAGScheduler: Broadcasting large task binary with size 1575.2 KiB\n",
      "24/06/10 09:22:23 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/06/10 09:22:30 WARN DAGScheduler: Broadcasting large task binary with size 1299.0 KiB\n",
      "24/06/10 09:22:32 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/06/10 09:22:34 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/06/10 09:22:38 WARN TaskSetManager: Stage 1194 contains a task of very large size (1164 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/10 09:22:40 WARN TaskSetManager: Stage 1219 contains a task of very large size (1164 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/10 09:22:56 WARN DAGScheduler: Broadcasting large task binary with size 1330.1 KiB\n",
      "24/06/10 09:23:02 WARN DAGScheduler: Broadcasting large task binary with size 1549.0 KiB\n",
      "24/06/10 09:23:03 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/06/10 09:23:09 WARN DAGScheduler: Broadcasting large task binary with size 1298.3 KiB\n",
      "24/06/10 09:23:10 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/06/10 09:23:12 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/06/10 09:23:20 WARN DAGScheduler: Broadcasting large task binary with size 1264.9 KiB\n",
      "24/06/10 09:23:26 WARN DAGScheduler: Broadcasting large task binary with size 1519.2 KiB\n",
      "24/06/10 09:23:27 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/06/10 09:23:34 WARN DAGScheduler: Broadcasting large task binary with size 1295.1 KiB\n",
      "24/06/10 09:23:36 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/06/10 09:23:38 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/06/10 09:23:42 WARN TaskSetManager: Stage 1590 contains a task of very large size (1164 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/10 09:23:44 WARN TaskSetManager: Stage 1615 contains a task of very large size (1164 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/10 09:24:00 WARN DAGScheduler: Broadcasting large task binary with size 1333.6 KiB\n",
      "24/06/10 09:24:06 WARN DAGScheduler: Broadcasting large task binary with size 1542.2 KiB\n",
      "24/06/10 09:24:07 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/06/10 09:24:13 WARN DAGScheduler: Broadcasting large task binary with size 1300.4 KiB\n",
      "24/06/10 09:24:14 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/06/10 09:24:16 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/06/10 09:24:23 WARN DAGScheduler: Broadcasting large task binary with size 1285.9 KiB\n",
      "24/06/10 09:24:29 WARN DAGScheduler: Broadcasting large task binary with size 1519.5 KiB\n",
      "24/06/10 09:24:31 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/06/10 09:24:38 WARN DAGScheduler: Broadcasting large task binary with size 1293.2 KiB\n",
      "24/06/10 09:24:39 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/06/10 09:24:42 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "24/06/10 09:24:46 WARN TaskSetManager: Stage 1986 contains a task of very large size (1164 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/10 09:24:50 WARN DAGScheduler: Broadcasting large task binary with size 1303.0 KiB\n",
      "24/06/10 09:24:51 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/06/10 09:24:54 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time_rf = time.time()\n",
    "rf_model_fitted = rf_cross_val.fit(train_dat)\n",
    "end_time_rf = time.time() - start_time_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac8b4c5-ec1b-4772-93a0-d5ab07188375",
   "metadata": {},
   "source": [
    "Get RMSE on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9ba10a9-07d1-4e66-80a6-1fc806ccc8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 09:24:57 WARN TaskSetManager: Stage 2024 contains a task of very large size (1164 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62.83659751992761"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RegressionEvaluator(labelCol = \"y\").evaluate(rf_model_fitted.transform(train_dat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694cfa0-90fc-4091-ae13-b68073e4ace4",
   "metadata": {},
   "source": [
    "Get RMSE on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70c492da-73ad-4ec9-bfa4-f1b41821df58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/10 09:24:57 WARN TaskSetManager: Stage 2028 contains a task of very large size (1164 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109.15383001545085"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RegressionEvaluator(labelCol = \"y\").evaluate(rf_model_fitted.transform(test_dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0493de8-cfb0-4065-8649-2d43575d4fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
